
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="Vision Transformer Adapters for Generalizable Multitask Learning">
    <meta name="author" content="Deblina Bhattacharjee, Sabine Süsstrunk, Mathieu Salzmann">

    <title>Vision Transformer Adapters for Generalizable Multitask Learning</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h1>Vision Transformer Adapters for Generalizable Multitask Learning</h1>
    <h3>ICCV 2023</h3>
           <p class="abstract"></p>
    <hr>
    <p class="authors">
        <a href="https://www.linkedin.com/in/deblina/"> Deblina Bhattacharjee</a>,
        <a href="https://www.epfl.ch/labs/ivrl/people/susstrunk/"> Sabine Süsstrunk</a>,
        <a href="https://people.epfl.ch/mathieu.salzmann"> Mathieu Salzmann</a>,
    </p>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/pdf/2205.08303.pdf">Paper</a>
        <!--<a class="btn btn-primary" href="https://github.com/IVRL/VTAGML">Code (coming soon)</a>-->
         <a class="btn btn-primary" href="https://drive.google.com/file/d/1Swf6LVL2Gi_gCwLxEhCpHg-5B8fG7Sor/view?usp=sharing">Slides</a>
    </div>
</div>

<div class="container">
    <div class="section">
        
                        <div class="row align-items-center">
                <div class="col justify-content-center text-center">
                    <img src="images/teaser-iccv1.pdf" style="width:90%; margin-right:-10px; margin-top:-10px;">
            </div> 
            <hr>
        <!--<div class="vcontainer">
            <iframe class='video' src="https://www.youtube.com/embed/_PgL3u6G_9E"  title="YouTube video player" frameborder="0" 
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>-->
        <p>
             We introduce the first multitasking vision transformer adapters that learn generalizable task affinities which can be applied to novel tasks and domains. Integrated into an off-the-shelf vision transformer backbone, our adapters can simultaneously solve multiple dense vision tasks in a parameter-efficient manner, unlike existing multitasking transformers that are parametrically expensive. In contrast to concurrent methods, we do not require retraining or fine-tuning whenever a new task or domain is added. We introduce a task-adapted attention mechanism within our adapter framework that combines gradient-based task similarities with attention-based ones. The learned task affinities generalize to the following settings: zero-shot task transfer, unsupervised domain adaptation, and generalization without fine-tuning to novel domains. We demonstrate that our approach outperforms not only the existing convolutional neural network-based multitasking methods but also the vision transformer-based ones.
        </div>

        <div class="section">
            <h2>Method Architecture</h2>
            <hr>
                        <div class="row align-items-center">
                <div class="col justify-content-center text-center">
                    <img src="images/overall-vision-adapter-architecture.png" style="width:100%; margin-right:-10px; margin-top:-10px;">
            </div> 
            <hr>
            <p>
                Detailed overview of our architecture. The frozen transformer encoder module (in <FONT COLOR="#FFA857"> orange</FONT>) extracts a shared representation of the input image, which is then utilized to learn the task affinities in our novel vision transformer adapters (in <FONT COLOR="#C39BD3">purple</FONT>). Each adapter layer uses gradient task similarity (TROA) (in <FONT COLOR="#F4D03F">yellow</FONT>) and Task-Adapted Attention (TAA) to learn the task affinities, which are communicated with skip connections (in <FONT COLOR="#2B4B9B">blue</FONT>) between consecutive adapter layers. The task embeddings are then decoded by the fully-supervised transformer decoders (in <FONT COLOR="#ABEBC6">green</FONT>) for the respective tasks. Note that the transformer decoders are shared but have different task heads (in <FONT COLOR="#9EA3A0">grey</FONT>). For clarity, only three tasks are depicted here and TAA is explained in a separate figure below.  
            </p>
        </div>
        </div>

            <h2>Vision Transformer Adapter Module</h2>
            
                        <div class="row align-items-center">
                <div class="col justify-content-center text-center">
                    <img src="images/vision-adapter-new.pdf" style="width:50%; margin-right:-10px; margin-top:-10px;">
            </div> 
            <hr>
            <p>
                Overview of our vision transformer adapter module. Our vision adapters learn transferable and generalizable task affinities in a parameter-efficient way. We show two blocks to depict the skip connectivity between them. The main modules (TROA) and (TAA) of our vision transformer adapters are depicted below.
            </p>
        </div>
        
        <h2>Task Represenattaion Optimization Algorithm (TROA)</h2>
            
                        <div class="row align-items-center">
                <div class="col justify-content-center text-center">
                    <img src="images/troa-new.png" style="width:50%; margin-right:-10px; margin-top:-10px;">
            </div> 
            <hr>
            <p>
                We show the task affinities from TROA when four tasks comprising semantic segmentation (SemSeg), depth, surface normal, and edges are jointly learned. We show that TROA learns a strong task affinity between the same task gradients, for example, segmentation with segmentation. This is a self-explanatory observation. Consequently, TROA also learns task affinities between proximate tasks such as segmentation and depth, and task affinities between non-proximate tasks such as segmentation and normal. Note that task dependence is assymetric, i.e. segmentation does not affect normal as normal effects segmentation. These task affinities are used by our novel task-adapted attention module as described in what follows.
            </p>
        </div>

        <h2>Matching the Feature Dimensions using FiLM</h2>
            
                        <div class="row align-items-center">
                <div class="col justify-content-center text-center">
                    <img src="images/film-finalised.pdf" style="width:80%; margin-right:-10px; margin-top:-10px;">
            </div> 
            <hr>
            <p>
                Detailed overview of Feature Wise Linear Modulation (FiLM)} which linearly shifts and scales tasks representations to match dimensions of the feature maps. The orange rectangular area is FiLM.  
            </p>
        </div>

        <h2>Task-Adapted Attention</h2>
            
                        <div class="row align-items-center">
                <div class="col justify-content-center text-center">
                    <img src="images/TAA-finalised.png" style="width:80%; margin-right:-10px; margin-top:-10px;">
            </div> 
            <hr>
            <p>
                Overview of our Task-Adapted Attention (TAA) mechanism that combines task affinities with image attention. Note that the process, in the foreground, is for a single attention head which is repeated for 'M' heads to give us the task-adapted multi-head attention.  
            </p>
        </div>

      
        <div class="section">
            <h2>Multitasking Results</h2>
            
            <hr>
                <div class="row align-items-center">
                <div class="col justify-content-center text-center">
                    <img src="images/nyu-qr-new.pdf" style="width:100%; margin-right:-10px; margin-top:-10px;">
            </div>
            <hr>
            <p>
             Multitask Learning comparison on the NYUDv2 benchmark in the'S-D-N-E' setting. Our model outperforms all the multitask baselines, i.e. ST-MTL, InvPT, Taskprompter, and MulT, respectively. For instance, our model correctly segments and predicts the surface normal of the elements within the yellow-circled region, unlike the baseline. All the methods are based on the same Swin-B V2 backbone. Best seen on screen and zoomed in. For more details and quantitative results, please refere to our paper.
            </p>
            </div>

               <div class="row align-items-center">
                <div class="col justify-content-center text-center">
                    <img src="images/qualitative-taskonomy.png" style="width:100%; margin-right:-10px; margin-top:-10px;">
            </div>
            <hr>
            <p>
             Multitask Learning comparison on the Taskonomy benchmark in the'S-D-N-E' setting. Our model outperforms all the multitask baselines, respectively. For instance, our model correctly segments and predicts the surface normal of the elements within the yellow-circled region, unlike the baseline. All the methods are based on the same Swin-B V2 backbone. Best seen on screen and zoomed in. For more details and quantitative results, please refere to our paper.
            </p>
            </div>
            </div>

    <div class="section">
            <h2>Unsupervised Domain Adaptation (UDA)</h2>
            <hr>
            
                <div class="row align-items-center">
                <div class="col justify-content-center text-center">
                    <img src="images/uda-results-new.pdf" style="width:90%; margin-right:-10px; margin-top:-10px;">
            </div>
            <hr>
            <p>
             Unsupervised Domain Adaptation (UDA) results on Synthia->Cityscapes.  Our model outperforms the CNN-based baseline (XTAM-UDA) and the Swin-B V2-based baselines (1-task Swin-UDA, MulT-UDA, respectively. For instance, our method can predict the depth of the car tail light, unlike the baselines. Best seen on screen and zoomed within the yellow circled region.
            </p>
            </div>
            </div>
    <div class="section">
        <h2>Bibtex</h2>
        <hr>
        <div class="row align-items-center"></div>
        <div class="col justify-content-center text-center"> 
        <div class="bibtexsection">
               @misc{
                    author = {Bhattacharjee, Deblina and Süsstrunk, Sabine and </br>                              Salzmann, Mathieu},
                    title = {Vision Transformer Adapters for Generalizable Multitask Learning},
                    publisher = {arXiv},
                    year = {2023},
                    copyright = {Creative Commons Attribution Non Commercial No Derivatives </br>                                4.0 International}
                    }

        </div>
    </div>
    </div>
<div class="section">
        <h2>Acknowledgement</h2>
        <div class="row align-items-center"></div>
        <div class="col justify-content-center text-center"> 
              <p> This work was supported in part by the Swiss National Science Foundation via the Sinergia grant CRSII5$-$180359.</p>

        </div>
    </div>
    </div>
    <hr>

</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>
</body>
</html>
